{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6fb387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-05-22 12:43:54.079406: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 12:43:54.730880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-22 12:43:55.887490: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-05-22 12:43:55.887525: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:134] retrieving CUDA diagnostic information for host: Nitro\n",
      "2024-05-22 12:43:55.887530: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:141] hostname: Nitro\n",
      "2024-05-22 12:43:55.887610: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:165] libcuda reported version is: 535.129.3\n",
      "2024-05-22 12:43:55.887627: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:169] kernel reported version is: 535.129.3\n",
      "2024-05-22 12:43:55.887631: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:248] kernel version seems to match DSO: 535.129.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Concatenate, TimeDistributed\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255f1646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date-time</th>\n",
       "      <th>Microwave</th>\n",
       "      <th>General</th>\n",
       "      <th>Refrigerator</th>\n",
       "      <th>Washing Machine</th>\n",
       "      <th>Shower</th>\n",
       "      <th>Air Fryer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03 18:43:45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-03 18:43:50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-03 18:43:55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-03 18:44:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-03 18:44:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>2022-10-15 10:33:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028901</td>\n",
       "      <td>0.170134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>2022-10-15 10:33:05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028857</td>\n",
       "      <td>0.169902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>2022-10-15 10:33:10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028873</td>\n",
       "      <td>0.169791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>2022-10-15 10:33:15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028820</td>\n",
       "      <td>0.169679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>2022-10-15 10:33:20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028976</td>\n",
       "      <td>0.169834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date-time  Microwave   General  Refrigerator  \\\n",
       "0       2022-10-03 18:43:45        0.0  0.007063      0.000000   \n",
       "1       2022-10-03 18:43:50        0.0  0.007219      0.000000   \n",
       "2       2022-10-03 18:43:55        0.0  0.007212      0.000000   \n",
       "3       2022-10-03 18:44:00        0.0  0.007201      0.000000   \n",
       "4       2022-10-03 18:44:05        0.0  0.007160      0.000000   \n",
       "...                     ...        ...       ...           ...   \n",
       "199995  2022-10-15 10:33:00        0.0  0.028901      0.170134   \n",
       "199996  2022-10-15 10:33:05        0.0  0.028857      0.169902   \n",
       "199997  2022-10-15 10:33:10        0.0  0.028873      0.169791   \n",
       "199998  2022-10-15 10:33:15        0.0  0.028820      0.169679   \n",
       "199999  2022-10-15 10:33:20        0.0  0.028976      0.169834   \n",
       "\n",
       "        Washing Machine  Shower  Air Fryer  \n",
       "0                   0.0     0.0        0.0  \n",
       "1                   0.0     0.0        0.0  \n",
       "2                   0.0     0.0        0.0  \n",
       "3                   0.0     0.0        0.0  \n",
       "4                   0.0     0.0        0.0  \n",
       "...                 ...     ...        ...  \n",
       "199995              0.0     0.0        0.0  \n",
       "199996              0.0     0.0        0.0  \n",
       "199997              0.0     0.0        0.0  \n",
       "199998              0.0     0.0        0.0  \n",
       "199999              0.0     0.0        0.0  \n",
       "\n",
       "[200000 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('scaled_training.csv')\n",
    "data = data.iloc[:200000,:]\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d849f5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values in date-time : 200000\n",
      "Unique Values in Microwave : 1\n",
      "Unique Values in General : 39382\n",
      "Unique Values in Refrigerator : 5008\n",
      "Unique Values in Washing Machine : 542\n",
      "Unique Values in Shower : 609\n",
      "Unique Values in Air Fryer : 1\n"
     ]
    }
   ],
   "source": [
    "for col in data.columns :\n",
    "    print(\"Unique Values in \" + col + \" :\" , data[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85ce31c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01008266 0.01287256 0.03321268 ... 0.03641411 0.03180699 0.02645533] [[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.17319069 0.         0.        ]\n",
      " ...\n",
      " [0.17197146 0.         0.        ]\n",
      " [0.17149064 0.         0.        ]\n",
      " [0.1688547  0.         0.        ]]\n",
      "[0.04206628 0.00768956 0.01043592 ... 0.02872209 0.04307455 0.01004096] [[0.17137902 0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " ...\n",
      " [0.16929259 0.         0.        ]\n",
      " [0.17204015 0.         0.        ]\n",
      " [0.         0.         0.        ]]\n",
      "[0.01841987 0.03070918 0.0304332  ... 0.0115325  0.03217497 0.04624041] [[0.         0.         0.        ]\n",
      " [0.16879459 0.         0.        ]\n",
      " [0.17298462 0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.        ]\n",
      " [0.17511398 0.         0.        ]\n",
      " [0.16985069 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#General Power Consumption Signal as Input\n",
    "x = data['General'].to_numpy()\n",
    "  \n",
    "# y are the Individual Appliance Signal which are to be predicted\n",
    "y = data.loc[:, ['Refrigerator','Washing Machine','Shower']].to_numpy()\n",
    "  \n",
    "# Splitting dataset in 70-15-15 fashion for Training,Testing and Validation DataSets\n",
    "x_train, x_Combine, y_train, y_Combine = train_test_split(x,y,test_size=0.3,random_state=25)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_Combine,y_Combine,test_size=0.5,random_state=42) \n",
    "\n",
    "print(x_train , y_train)\n",
    "print(x_val , y_val)\n",
    "print(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87154d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000,) (140000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape , y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e834cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (139901, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "window_size = 100  # Number of time steps in each window\n",
    "\n",
    "# Create sliding windows\n",
    "def create_sliding_windows(data, window_size):\n",
    "    windows = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        windows.append(data[i:i+window_size])\n",
    "    return np.array(windows)\n",
    "\n",
    "x_train = create_sliding_windows(x_train, window_size)\n",
    "x_train = np.expand_dims(x_train, axis=2)  # Add feature dimension\n",
    "\n",
    "# Example shapes\n",
    "print(\"Input shape:\", x_train.shape)  # (number_of_windows, window_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94444d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = create_sliding_windows(x_val, window_size)\n",
    "x_val = np.expand_dims(x_val, axis=2)\n",
    "x_test = create_sliding_windows(x_test, window_size)\n",
    "x_test = np.expand_dims(x_test, axis=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b065ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29901, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape)  # (number_of_windows, window_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20594250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7fcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows_y(data, window_size):\n",
    "    num_windows = len(data) - window_size + 1\n",
    "    shape = (num_windows, window_size, data.shape[1])\n",
    "    strides = (data.strides[0], data.strides[0], data.strides[1])\n",
    "    windows = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0bb3ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape: (139901, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "y_train = create_sliding_windows_y(y_train, window_size)\n",
    "print(\"Target shape:\", y_train.shape)  # (number_of_windows, window_size, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "329f101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape: (29901, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "y_val = create_sliding_windows_y(y_val, window_size)\n",
    "print(\"Target shape:\", y_val.shape)  # (number_of_windows, window_size, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4e9a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape: (29901, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "y_test = create_sliding_windows_y(y_test, window_size)\n",
    "print(\"Target shape:\", y_test.shape)  # (number_of_windows, window_size, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f88e5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Attention layer\n",
    "# class AttentionLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n",
    "#                                  initializer='random_normal', trainable=True)\n",
    "#         self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "#                                  initializer='random_normal', trainable=True)\n",
    "#         self.V = self.add_weight(shape=(input_shape[-1], 1),\n",
    "#                                  initializer='random_normal', trainable=True)\n",
    "#         super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "#         attention_weights = tf.nn.softmax(tf.tensordot(score, self.V, axes=1), axis=1)\n",
    "#         context_vector = attention_weights * inputs\n",
    "#         context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "#         return context_vector, attention_weights\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], input_shape[-1]), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[-1],), initializer='zeros', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class ExpandDimsLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, axis=1)\n",
    "\n",
    "    \n",
    "def create_model(input_shape, num_outputs):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    encoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(inputs)\n",
    "\n",
    "    # Attention\n",
    "    context_vector, attention_weights = AttentionLayer()(encoder_outputs)\n",
    "    \n",
    "    # ExpandDims Layer\n",
    "    context_vector = ExpandDimsLayer()(context_vector)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(context_vector, initial_state=[state_h, state_c])\n",
    "\n",
    "    outputs = TimeDistributed(Dense(num_outputs ,activation='relu'))(decoder_outputs)  # Ensure correct output shape\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Define window size, number of features and number of outputs\n",
    "window_size = 100\n",
    "num_features = 1  # Assuming 1 feature for total power\n",
    "num_outputs = 3  # Assuming 3 appliances\n",
    "\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Create the model\n",
    "input_shape = (window_size, num_features)\n",
    "model = create_model(input_shape, num_outputs)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec4b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m   1/4372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25:42\u001b[0m 2s/step - loss: 0.0045 - mae: 0.0263"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adeem/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 60ms/step - loss: 0.0033 - mae: 0.0293 - val_loss: 0.0034 - val_mae: 0.0295\n",
      "Epoch 2/50\n",
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 60ms/step - loss: 0.0033 - mae: 0.0293 - val_loss: 0.0034 - val_mae: 0.0294\n",
      "Epoch 3/50\n",
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 60ms/step - loss: 0.0033 - mae: 0.0293 - val_loss: 0.0034 - val_mae: 0.0295\n",
      "Epoch 4/50\n",
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 61ms/step - loss: 0.0033 - mae: 0.0293 - val_loss: 0.0034 - val_mae: 0.0295\n",
      "Epoch 5/50\n",
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 62ms/step - loss: 0.0033 - mae: 0.0293 - val_loss: 0.0034 - val_mae: 0.0294\n",
      "Epoch 6/50\n",
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 61ms/step - loss: 0.0033 - mae: 0.0293 - val_loss: 0.0034 - val_mae: 0.0295\n",
      "Epoch 7/50\n",
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 61ms/step - loss: 0.0034 - mae: 0.0293 - val_loss: 0.0034 - val_mae: 0.0293\n",
      "Epoch 8/50\n",
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 60ms/step - loss: 0.0033 - mae: 0.0292 - val_loss: 0.0034 - val_mae: 0.0293\n",
      "Epoch 9/50\n",
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 59ms/step - loss: 0.0033 - mae: 0.0292 - val_loss: 0.0034 - val_mae: 0.0293\n",
      "Epoch 10/50\n",
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 59ms/step - loss: 0.0033 - mae: 0.0292 - val_loss: 0.0034 - val_mae: 0.0294\n",
      "Epoch 11/50\n",
      "\u001b[1m4372/4372\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 59ms/step - loss: 0.0033 - mae: 0.0292 - val_loss: 0.0034 - val_mae: 0.0293\n",
      "Epoch 12/50\n",
      "\u001b[1m 476/4372\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:33\u001b[0m 55ms/step - loss: 0.0032 - mae: 0.0290"
     ]
    }
   ],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.x))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_x = self.x[batch_indexes]\n",
    "        batch_y = self.y[batch_indexes]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "# Ensure data shapes are correct\n",
    "x_train = x_train.reshape(-1, window_size, num_features)\n",
    "y_train = y_train.reshape(-1, window_size, num_outputs)\n",
    "x_val = x_val.reshape(-1, window_size, num_features)\n",
    "y_val = y_val.reshape(-1, window_size, num_outputs)\n",
    "x_test = x_test.reshape(-1, window_size, num_features)\n",
    "y_test = y_test.reshape(-1, window_size, num_outputs)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = DataGenerator(x_train, y_train, batch_size=32)\n",
    "val_generator = DataGenerator(x_val, y_val, batch_size=32)\n",
    "test_generator = DataGenerator(x_test, y_test, batch_size=32)\n",
    "\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model_checkpoint.keras',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "\n",
    "#Train the model with callbacks\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=50,\n",
    "                    validation_data=val_generator,\n",
    "                    callbacks=[checkpoint_callback, tensorboard_callback])\n",
    "\n",
    "# Train the model\n",
    "#history = model.fit(train_generator, epochs=50, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_mae = model.evaluate(test_dataset)\n",
    "print(f'Test MAE: {test_mae}')\n",
    "\n",
    "# Predict individual appliance consumption\n",
    "y_pred = model.predict(test_dataset)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('updated_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(new_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a84ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82692700",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df = pd.DataFrame(y_test)\n",
    "display(y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in pred_df.columns:\n",
    "    plt.figure(figsize=(16, 8), dpi=150)\n",
    "    pred_df[col].head(50000).plot(label=col, color='red')\n",
    "    plt.title('Power Consumption Plot')\n",
    "    plt.xlabel('TimeStamp')\n",
    "    plt.ylabel(col)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ba711",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in y_test_df.columns:\n",
    "    plt.figure(figsize=(16, 8), dpi=150)\n",
    "    y_test_df[col].head(50000).plot(label=col, color='green')\n",
    "    plt.title('Power Consumption Plot')\n",
    "    plt.xlabel('TimeStamp')\n",
    "    plt.ylabel(col)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
